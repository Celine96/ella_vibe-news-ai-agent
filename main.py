import logging
import os
import asyncio
import time
from datetime import datetime
from typing import Optional, Any
import uuid
from collections import deque
import re
import json

from fastapi import FastAPI
from pydantic import BaseModel
from openai import OpenAI, OpenAIError, APITimeoutError
import numpy as np
import pickle

# ë‰´ìŠ¤ í¬ë¡¤ë§ìš©
import requests
from bs4 import BeautifulSoup

# ğŸ†• ë‰´ìŠ¤ í•„í„°ë§ ì‹œìŠ¤í…œ
try:
    from news_filter_simple import filter_real_estate_news, filter_news_batch
    NEWS_FILTER_AVAILABLE = True
except ImportError:
    NEWS_FILTER_AVAILABLE = False
    logging.warning("âš ï¸ news_filter_simple.py not found - filtering disabled")

# Redis for queue management
try:
    import redis.asyncio as redis
    from redis.asyncio import Redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    Redis = Any
    logging.warning("redis package not installed. Using in-memory queue.")

# ================================================================================
# Logging Configuration
# ================================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="REXA - Real Estate Expert Assistant",
    description="Solar API + RAG chatbot for real estate + News QA",
    version="2.0.0"
)

# ================================================================================
# Configuration & Global Variables
# ================================================================================

# Naver News API
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# Redis Configuration
REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
REDIS_DB = int(os.getenv("REDIS_DB", 0))
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD", None)

# Health Check Configuration
HEALTH_CHECK_INTERVAL = int(os.getenv("HEALTH_CHECK_INTERVAL", 5))
MAX_UNHEALTHY_COUNT = int(os.getenv("MAX_UNHEALTHY_COUNT", 3))

# Queue Configuration
WEBHOOK_QUEUE_NAME = "rexa:webhook_queue"
WEBHOOK_PROCESSING_QUEUE = "rexa:processing_queue"
WEBHOOK_FAILED_QUEUE = "rexa:failed_queue"
MAX_RETRY_ATTEMPTS = int(os.getenv("MAX_RETRY_ATTEMPTS", 3))
QUEUE_PROCESS_INTERVAL = int(os.getenv("QUEUE_PROCESS_INTERVAL", 5))

# API Timeout Configuration
API_TIMEOUT = int(os.getenv("API_TIMEOUT", 3))

# Global state
redis_client: Optional[Any] = None
server_healthy = True
unhealthy_count = 0
last_health_check = datetime.now()

# In-memory queue fallback
in_memory_webhook_queue: deque = deque()
in_memory_processing_queue: deque = deque()
in_memory_failed_queue: deque = deque()
use_in_memory_queue = False

# News session storage (user_id -> news_data)
news_sessions = {}

# ================================================================================
# Upstage Solar API Client (with timeout)
# ================================================================================

SOLAR_API_KEY = os.getenv("SOLAR_API_KEY", "")
SOLAR_API_BASE = "https://api.upstage.ai/v1/solar"

client = OpenAI(
    api_key=SOLAR_API_KEY,
    base_url=SOLAR_API_BASE,
    timeout=API_TIMEOUT
)

logger.info(f"âœ… Solar API client initialized (Timeout: {API_TIMEOUT}s)")

# ================================================================================
# RAG System - Load embeddings and chunks
# ================================================================================

# ì„ë² ë”© ë° ì²­í¬ ë°ì´í„° ë¡œë“œ (ì „ì—­)
try:
    with open("chunk_embeddings.pkl", "rb") as f:
        chunk_embeddings = pickle.load(f)
    logger.info(f"âœ… Loaded {len(chunk_embeddings)} chunk embeddings")
except FileNotFoundError:
    chunk_embeddings = []
    logger.warning("âš ï¸ chunk_embeddings.pkl not found - RAG disabled")

try:
    with open("article_chunks.pkl", "rb") as f:
        article_chunks = pickle.load(f)
    logger.info(f"âœ… Loaded {len(article_chunks)} article chunks")
except FileNotFoundError:
    article_chunks = []
    logger.warning("âš ï¸ article_chunks.pkl not found - RAG disabled")

def retrieve_relevant_chunks(question: str, top_k: int = 3):
    """
    ì§ˆë¬¸ ì„ë² ë”©ì„ ìƒì„±í•˜ê³ , ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ìƒìœ„ kê°œ ì²­í¬ ë°˜í™˜
    """
    if len(chunk_embeddings) == 0:
        logger.warning("âš ï¸ No embeddings loaded - RAG retrieve failed")
        return []
    
    try:
        # Solar embedding ëª¨ë¸ ì‚¬ìš©
        response = client.embeddings.create(
            model="solar-embedding-1-large-query",
            input=question
        )
        q_emb = np.array(response.data[0].embedding)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        scores = []
        for i, c_emb in enumerate(chunk_embeddings):
            sim = np.dot(q_emb, c_emb) / (np.linalg.norm(q_emb) * np.linalg.norm(c_emb) + 1e-9)
            scores.append((i, sim))
        
        scores.sort(key=lambda x: x[1], reverse=True)
        top_indices = [idx for idx, _ in scores[:top_k]]
        return [article_chunks[i] for i in top_indices]
    
    except Exception as e:
        logger.error(f"âŒ RAG retrieve error: {e}")
        return []

# ================================================================================
# Naver News API - Search
# ================================================================================

def search_naver_news(query: str, display: int = 5):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰"""
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        logger.warning("âš ï¸ Naver API credentials not configured")
        return []
    
    try:
        url = "https://openapi.naver.com/v1/search/news.json"
        headers = {
            "X-Naver-Client-Id": NAVER_CLIENT_ID,
            "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
        }
        params = {
            "query": query,
            "display": display,
            "sort": "date"
        }
        
        response = requests.get(url, headers=headers, params=params, timeout=3)
        response.raise_for_status()
        
        data = response.json()
        items = data.get("items", [])
        
        # HTML íƒœê·¸ ì œê±°
        for item in items:
            item['title'] = re.sub(r'<[^>]+>', '', item['title'])
            item['description'] = re.sub(r'<[^>]+>', '', item['description'])
        
        logger.info(f"âœ… Found {len(items)} news articles for: {query}")
        return items
        
    except Exception as e:
        logger.error(f"âŒ Naver News API error: {e}")
        return []

# ================================================================================
# Pydantic Models
# ================================================================================

class UserRequest(BaseModel):
    userRequest: dict
    bot: Optional[dict] = None
    action: Optional[dict] = None
    contexts: Optional[list] = None

class HealthStatus(BaseModel):
    status: str
    model: str
    mode: str
    server_healthy: bool
    last_check: str
    redis_connected: bool
    queue_size: int
    processing_queue_size: int
    failed_queue_size: int

class QueuedRequest(BaseModel):
    request_id: str
    payload: dict
    retry_count: int = 0
    created_at: datetime
    processing_started_at: Optional[datetime] = None

# ================================================================================
# Redis Queue Management
# ================================================================================

async def init_redis():
    """Initialize Redis connection with fallback to in-memory queue"""
    global redis_client, use_in_memory_queue
    
    if not REDIS_AVAILABLE:
        logger.warning("âš ï¸ Redis not installed - using in-memory queue")
        use_in_memory_queue = True
        return
    
    try:
        redis_client = await redis.from_url(
            f"redis://{REDIS_HOST}:{REDIS_PORT}/{REDIS_DB}",
            password=REDIS_PASSWORD,
            encoding="utf-8",
            decode_responses=True
        )
        await redis_client.ping()
        logger.info(f"âœ… Redis connected: {REDIS_HOST}:{REDIS_PORT}")
        use_in_memory_queue = False
    except Exception as e:
        logger.warning(f"âš ï¸ Redis connection failed: {e}")
        logger.warning("âš ï¸ Using in-memory queue as fallback")
        redis_client = None
        use_in_memory_queue = True

async def close_redis():
    """Close Redis connection"""
    global redis_client
    if redis_client:
        await redis_client.close()
        logger.info("âœ… Redis connection closed")

async def enqueue_webhook_request(request_id: str, payload: dict):
    """Enqueue a webhook request for later processing"""
    request = QueuedRequest(
        request_id=request_id,
        payload=payload,
        created_at=datetime.now()
    )
    
    if use_in_memory_queue:
        in_memory_webhook_queue.append(request)
        logger.info(f"âœ… Request {request_id[:8]} enqueued (in-memory, size: {len(in_memory_webhook_queue)})")
        return
    
    if not redis_client:
        logger.error("âŒ Queue not available")
        return
    
    try:
        await redis_client.lpush(WEBHOOK_QUEUE_NAME, request.model_dump_json())
        queue_size = await redis_client.llen(WEBHOOK_QUEUE_NAME)
        logger.info(f"âœ… Request {request_id[:8]} enqueued (Redis, size: {queue_size})")
    except Exception as e:
        logger.error(f"âŒ Failed to enqueue request: {e}")

async def dequeue_webhook_request() -> Optional[QueuedRequest]:
    """Dequeue and process next webhook request"""
    if use_in_memory_queue:
        if len(in_memory_webhook_queue) == 0:
            return None
        request = in_memory_webhook_queue.popleft()
        in_memory_processing_queue.append(request)
        return request
    
    if not redis_client:
        return None
    
    try:
        # Move from webhook queue to processing queue
        data = await redis_client.rpoplpush(WEBHOOK_QUEUE_NAME, WEBHOOK_PROCESSING_QUEUE)
        if not data:
            return None
        
        request = QueuedRequest.model_validate_json(data)
        request.processing_started_at = datetime.now()
        return request
    except Exception as e:
        logger.error(f"âŒ Failed to dequeue request: {e}")
        return None

async def mark_request_completed(request: QueuedRequest):
    """Remove request from processing queue after successful completion"""
    if use_in_memory_queue:
        try:
            in_memory_processing_queue.remove(request)
        except ValueError:
            pass
        return
    
    if not redis_client:
        return
    
    try:
        await redis_client.lrem(WEBHOOK_PROCESSING_QUEUE, 1, request.model_dump_json())
    except Exception as e:
        logger.error(f"âŒ Failed to mark request as completed: {e}")

async def mark_request_failed(request: QueuedRequest):
    """Move request to failed queue after max retries"""
    if use_in_memory_queue:
        try:
            in_memory_processing_queue.remove(request)
        except ValueError:
            pass
        in_memory_failed_queue.append(request)
        return
    
    if not redis_client:
        return
    
    try:
        await redis_client.lrem(WEBHOOK_PROCESSING_QUEUE, 1, request.model_dump_json())
        await redis_client.lpush(WEBHOOK_FAILED_QUEUE, request.model_dump_json())
    except Exception as e:
        logger.error(f"âŒ Failed to mark request as failed: {e}")

async def requeue_request(request: QueuedRequest):
    """Put request back in queue for retry"""
    request.retry_count += 1
    
    if use_in_memory_queue:
        try:
            in_memory_processing_queue.remove(request)
        except ValueError:
            pass
        in_memory_webhook_queue.appendleft(request)
        return
    
    if not redis_client:
        return
    
    try:
        await redis_client.lrem(WEBHOOK_PROCESSING_QUEUE, 1, request.model_dump_json())
        await redis_client.lpush(WEBHOOK_QUEUE_NAME, request.model_dump_json())
    except Exception as e:
        logger.error(f"âŒ Failed to requeue request: {e}")

async def get_queue_sizes() -> tuple[int, int, int]:
    """Get sizes of all queues"""
    if use_in_memory_queue:
        return (
            len(in_memory_webhook_queue),
            len(in_memory_processing_queue),
            len(in_memory_failed_queue)
        )
    
    if not redis_client:
        return (0, 0, 0)
    
    try:
        queue_size = await redis_client.llen(WEBHOOK_QUEUE_NAME)
        processing_size = await redis_client.llen(WEBHOOK_PROCESSING_QUEUE)
        failed_size = await redis_client.llen(WEBHOOK_FAILED_QUEUE)
        return (queue_size, processing_size, failed_size)
    except Exception as e:
        logger.error(f"âŒ Failed to get queue sizes: {e}")
        return (0, 0, 0)

# ================================================================================
# Background Workers
# ================================================================================

async def health_check_monitor():
    """Monitor server health"""
    global server_healthy, unhealthy_count, last_health_check
    
    while True:
        try:
            await asyncio.sleep(HEALTH_CHECK_INTERVAL)
            
            # Check if we can still make API calls
            try:
                test_response = client.chat.completions.create(
                    model="solar-mini",
                    messages=[{"role": "user", "content": "test"}],
                    max_tokens=5,
                    timeout=2
                )
                server_healthy = True
                unhealthy_count = 0
            except Exception as e:
                unhealthy_count += 1
                if unhealthy_count >= MAX_UNHEALTHY_COUNT:
                    server_healthy = False
                    logger.error(f"âŒ Server unhealthy: {unhealthy_count} consecutive failures")
            
            last_health_check = datetime.now()
            
        except Exception as e:
            logger.error(f"âŒ Health check monitor error: {e}")

async def queue_processor():
    """Background worker to process queued requests"""
    logger.info("ğŸ”„ Queue processor started")
    
    while True:
        try:
            await asyncio.sleep(QUEUE_PROCESS_INTERVAL)
            
            request = await dequeue_webhook_request()
            if not request:
                continue
            
            logger.info(f"ğŸ”„ Processing queued request {request.request_id[:8]} (attempt {request.retry_count + 1})")
            
            try:
                # Process the request
                result = await process_solar_rag_request(request.payload)
                await mark_request_completed(request)
                logger.info(f"âœ… Queued request {request.request_id[:8]} completed")
                
            except Exception as e:
                logger.error(f"âŒ Queued request {request.request_id[:8]} failed: {e}")
                
                if request.retry_count < MAX_RETRY_ATTEMPTS - 1:
                    await requeue_request(request)
                    logger.info(f"ğŸ”„ Requeued request {request.request_id[:8]} (retry {request.retry_count + 1})")
                else:
                    await mark_request_failed(request)
                    logger.error(f"âŒ Request {request.request_id[:8]} moved to failed queue")
        
        except Exception as e:
            logger.error(f"âŒ Queue processor error: {e}")

# ================================================================================
# Main Logic - Process Solar RAG Request
# ================================================================================

async def process_solar_rag_request(request_body: dict):
    """
    ì‹¤ì œ ìš”ì²­ ì²˜ë¦¬: Solar + RAG + ë‰´ìŠ¤
    """
    utterance_raw = request_body.get("userRequest", {}).get("utterance", "").strip()
    user_id = request_body.get("userRequest", {}).get("user", {}).get("id", "unknown")
    
    logger.info(f"ğŸ‘¤ User: {user_id}")
    logger.info(f"ğŸ’¬ Question: {utterance_raw}")
    
    # === ë‰´ìŠ¤ ë‹µë³€ ëª¨ë“œ ì²´í¬ ===
    if user_id in news_sessions:
        news_data = news_sessions[user_id]
        question = utterance_raw.lower()
        
        # ë‰´ìŠ¤ ë²ˆí˜¸ ì¶”ì¶œ (1-5)
        match = re.search(r'(\d+)', question)
        if match:
            news_idx = int(match.group(1)) - 1
            if 0 <= news_idx < len(news_data):
                news = news_data[news_idx]
                
                # ë§í¬ í¬ë¡¤ë§
                article_text = crawl_article(news['link'])
                if not article_text:
                    article_text = news['description']
                
                # GPTë¡œ ë‹µë³€ ìƒì„±
                prompt = f"""ë‹¤ìŒì€ ë¶€ë™ì‚° ë‰´ìŠ¤ ê¸°ì‚¬ì…ë‹ˆë‹¤:

ì œëª©: {news['title']}
ë‚´ìš©: {article_text}

ì‚¬ìš©ì ì§ˆë¬¸: {utterance_raw}

ìœ„ ê¸°ì‚¬ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ì‘ì„±í•˜ë˜, 200ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ í•´ì£¼ì„¸ìš”."""

                try:
                    response = client.chat.completions.create(
                        model="solar-mini",
                        messages=[{"role": "user", "content": prompt}],
                        timeout=API_TIMEOUT
                    )
                    answer = response.choices[0].message.content.strip()
                    
                    # ì„¸ì…˜ ì¢…ë£Œ
                    del news_sessions[user_id]
                    
                    return {
                        "version": "2.0",
                        "template": {
                            "outputs": [
                                {"simpleText": {"text": answer}}
                            ]
                        }
                    }
                except Exception as e:
                    logger.error(f"âŒ GPT answer error: {e}")
                    del news_sessions[user_id]
                    return {
                        "version": "2.0",
                        "template": {
                            "outputs": [
                                {"simpleText": {"text": "ê¸°ì‚¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}}
                            ]
                        }
                    }
        
        # ì˜ëª»ëœ ì…ë ¥
        del news_sessions[user_id]
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {"simpleText": {"text": "ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” (ì˜ˆ: 1ë²ˆ)"}}
                ]
            }
        }
    
    # === ë‰´ìŠ¤ ê²€ìƒ‰ ìš”ì²­ ì²˜ë¦¬ ===
    news_keywords = ["ë‰´ìŠ¤", "ìµœê·¼", "ê¸°ì‚¬", "ì†Œì‹"]
    if any(kw in utterance_raw for kw in news_keywords):
        # ê²€ìƒ‰ì–´ ì¶”ì¶œ
        query = utterance_raw
        for kw in news_keywords:
            query = query.replace(kw, "").strip()
        
        if not query:
            query = "ë¶€ë™ì‚°"
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
        news_items = search_naver_news(query, display=10)
        
        if not news_items:
            return {
                "version": "2.0",
                "template": {
                    "outputs": [
                        {"simpleText": {"text": f"'{query}' ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}}
                    ]
                }
            }
        
        # ğŸ†• ë‰´ìŠ¤ í•„í„°ë§ (GPT-4o-mini)
        if NEWS_FILTER_AVAILABLE:
            filtered_news = filter_news_batch(news_items)
            # ê´€ë ¨ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
            filtered_news.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
            # ìƒìœ„ 5ê°œë§Œ
            filtered_news = filtered_news[:5]
        else:
            # í•„í„°ë§ ì—†ì´ ìƒìœ„ 5ê°œ
            filtered_news = news_items[:5]
        
        if not filtered_news:
            return {
                "version": "2.0",
                "template": {
                    "outputs": [
                        {"simpleText": {"text": f"'{query}' ê´€ë ¨ ë¶€ë™ì‚° ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}}
                    ]
                }
            }
        
        # ë‰´ìŠ¤ ì„¸ì…˜ ì €ì¥
        news_sessions[user_id] = filtered_news
        
        # ë‰´ìŠ¤ ëª©ë¡ í…ìŠ¤íŠ¸ ìƒì„±
        news_text = f"'{query}' ê´€ë ¨ ë¶€ë™ì‚° ë‰´ìŠ¤ {len(filtered_news)}ê±´ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n"
        for i, news in enumerate(filtered_news, 1):
            relevance = news.get('relevance_score', 0)
            keywords_str = ', '.join(news.get('keywords', [])[:3]) if news.get('keywords') else ''
            
            news_text += f"{i}. {news['title']}\n"
            if keywords_str:
                news_text += f"   í‚¤ì›Œë“œ: {keywords_str}\n"
            if NEWS_FILTER_AVAILABLE and relevance > 0:
                news_text += f"   ê´€ë ¨ë„: {relevance}ì \n"
            news_text += "\n"
        
        news_text += "ìì„¸íˆ ì•Œê³  ì‹¶ì€ ë‰´ìŠ¤ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” (ì˜ˆ: 1ë²ˆ)"
        
        logger.info(f"âœ… Found {len(filtered_news)} relevant news articles")
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {"simpleText": {"text": news_text}}
                ]
            }
        }
    
    # === ì¼ë°˜ ì§ˆë¬¸ (RAG + Solar) ===
    rag_context = ""
    if len(chunk_embeddings) > 0:
        top_chunks = retrieve_relevant_chunks(utterance_raw, top_k=3)
        if top_chunks:
            rag_context = "\n\n".join(top_chunks)
            logger.info(f"âœ… RAG: Retrieved {len(top_chunks)} chunks")
    
    system_msg = """ë‹¹ì‹ ì€ REXA, ë¶€ë™ì‚° ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ê¸ˆí•˜ë¹Œë”©(ì„œìš¸ ê°•ë‚¨êµ¬ ë…¼í˜„ë™ 21-1)ì— ëŒ€í•œ ì „ë¬¸ ì§€ì‹ì„ ë³´ìœ í•˜ê³  ìˆìœ¼ë©°, 
ìƒì—…ìš© ë¶€ë™ì‚° ì„ëŒ€ì°¨ë³´í˜¸ë²•, ì–‘ë„ì†Œë“ì„¸, ì‹œì„¸ ë¶„ì„ ë“±ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

ë‹µë³€ ì›ì¹™:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ìš°ì„  í™œìš©
2. ì „ë¬¸ì ì´ë©´ì„œë„ ì¹œê·¼í•œ í†¤
3. êµ¬ì²´ì  ìˆ˜ì¹˜ë‚˜ ë²•ë¥ ì€ ì •í™•í•˜ê²Œ
4. 200ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ"""

    if rag_context:
        user_msg = f"""[ì°¸ê³  ë¬¸ì„œ]
{rag_context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{utterance_raw}"""
    else:
        user_msg = utterance_raw
    
    try:
        response = client.chat.completions.create(
            model="solar-mini",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg}
            ],
            timeout=API_TIMEOUT
        )
        
        answer_text = response.choices[0].message.content.strip()
        logger.info(f"âœ… Solar response generated")
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {"simpleText": {"text": answer_text}}
                ]
            }
        }
    
    except APITimeoutError:
        logger.warning(f"â° Solar API timeout after {API_TIMEOUT}s")
        raise
    except OpenAIError as e:
        logger.error(f"âŒ Solar API error: {e}")
        raise
    except Exception as e:
        logger.error(f"âŒ Unexpected error: {type(e).__name__}: {e}")
        raise

def crawl_article(url: str) -> str:
    """ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§"""
    try:
        response = requests.get(url, timeout=3)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤
        article = soup.find('article')
        if article:
            return article.get_text(strip=True)
        
        # ì¼ë°˜ ê¸°ì‚¬
        paragraphs = soup.find_all('p')
        if paragraphs:
            return ' '.join([p.get_text(strip=True) for p in paragraphs[:10]])
        
        return ""
    except Exception as e:
        logger.error(f"âŒ Crawl error: {e}")
        return ""

# ================================================================================
# FastAPI Endpoints
# ================================================================================

@app.post("/webhook/solar-rag")
async def webhook_endpoint(request: UserRequest):
    """
    ì¹´ì¹´ì˜¤í†¡ ì±—ë´‡ ì›¹í›… ì—”ë“œí¬ì¸íŠ¸
    """
    request_id = str(uuid.uuid4())
    
    logger.info("="*50)
    logger.info(f"ğŸ“¨ New RAG request received: {request_id[:8]}")
    logger.info(f"ğŸ“‹ Full request body: {request.model_dump()}")
    
    try:
        # 3ì´ˆ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë¹ ë¥¸ ì‘ë‹µ ì‹œë„
        result = await process_solar_rag_request(request.model_dump())
        logger.info(f"âœ… Request {request_id[:8]} completed successfully")
        return result
        
    except APITimeoutError as e:
        logger.warning(f"â° Timeout (3s) - enqueueing request {request_id}")
        await enqueue_webhook_request(request_id, request.model_dump())
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": "ë‹µë³€ ìƒì„±ì— ì‹œê°„ì´ ê±¸ë¦¬ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                        }
                    }
                ]
            }
        }
        
    except OpenAIError as e:
        logger.error(f"âŒ API Error: {e}")
        await enqueue_webhook_request(request_id, request.model_dump())
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": "ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                        }
                    }
                ]
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Error: {type(e).__name__}: {e}")
        await enqueue_webhook_request(request_id, request.model_dump())
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": "ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ í•œë²ˆ ì§ˆë¬¸í•´ì£¼ì‹œê² ì–´ìš”?"
                        }
                    }
                ]
            }
        }

@app.get("/health")
async def health_check() -> HealthStatus:
    """Enhanced health check endpoint"""
    queue_size, processing_size, failed_size = await get_queue_sizes()
    
    return HealthStatus(
        status="healthy" if server_healthy else "unhealthy",
        model="solar-mini",
        mode="rexa_chatbot_rag_news",
        server_healthy=server_healthy,
        last_check=last_health_check.isoformat(),
        redis_connected=(redis_client is not None and not use_in_memory_queue),
        queue_size=queue_size,
        processing_queue_size=processing_size,
        failed_queue_size=failed_size
    )

@app.get("/health/ping")
async def health_ping():
    """Simple ping endpoint for client health checks"""
    return {
        "alive": True,
        "healthy": server_healthy,
        "timestamp": datetime.now().isoformat(),
        "rag_enabled": len(chunk_embeddings) > 0,
        "news_sessions": len(news_sessions)
    }

@app.get("/queue/status")
async def queue_status():
    """Get detailed queue status"""
    queue_size, processing_size, failed_size = await get_queue_sizes()
    
    return {
        "queue_type": "in-memory" if use_in_memory_queue else "redis",
        "webhook_queue": queue_size,
        "processing_queue": processing_size,
        "failed_queue": failed_size,
        "total": queue_size + processing_size + failed_size,
        "rag_chunks_loaded": len(article_chunks),
        "active_news_sessions": len(news_sessions)
    }

@app.post("/queue/retry-failed")
async def retry_failed_requests():
    """Manually retry all failed requests"""
    try:
        if use_in_memory_queue:
            retry_count = len(in_memory_failed_queue)
            while len(in_memory_failed_queue) > 0:
                req = in_memory_failed_queue.pop()
                req.retry_count = 0
                in_memory_webhook_queue.appendleft(req)
            
            logger.info(f"âœ… Retrying {retry_count} failed requests (in-memory)")
            return {"retried": retry_count, "queue_type": "in-memory"}
        
        if not redis_client:
            return {"error": "Queue not available"}
        
        failed_items = await redis_client.lrange(WEBHOOK_FAILED_QUEUE, 0, -1)
        retry_count = 0
        
        for item in failed_items:
            req = QueuedRequest.model_validate_json(item)
            req.retry_count = 0
            await redis_client.lpush(WEBHOOK_QUEUE_NAME, req.model_dump_json())
            retry_count += 1
        
        await redis_client.delete(WEBHOOK_FAILED_QUEUE)
        
        logger.info(f"âœ… Retrying {retry_count} failed requests (Redis)")
        return {"retried": retry_count, "queue_type": "redis"}
        
    except Exception as e:
        logger.error(f"âŒ Failed to retry requests: {e}")
        return {"error": str(e)}

# ================================================================================
# Startup & Shutdown Events
# ================================================================================

@app.on_event("startup")
async def startup_event():
    """Initialize resources on startup"""
    logger.info("="*70)
    logger.info("ğŸš€ Starting REXA server (Solar + RAG + News + Filtering)...")
    logger.info("="*70)
    
    # RAG ìƒíƒœ í™•ì¸
    if len(chunk_embeddings) > 0:
        logger.info(f"âœ… RAG ENABLED: {len(chunk_embeddings)} chunks loaded")
    else:
        logger.warning("âš ï¸ RAG DISABLED: No embeddings loaded")
        logger.warning("âš ï¸ Server will work but without company-specific knowledge")
    
    # Naver API í™•ì¸
    if NAVER_CLIENT_ID and NAVER_CLIENT_SECRET:
        logger.info("âœ… Naver News API configured")
    else:
        logger.warning("âš ï¸ Naver News API not configured")
    
    # ğŸ†• News Filtering í™•ì¸
    if NEWS_FILTER_AVAILABLE:
        logger.info("âœ… News filtering system enabled")
    else:
        logger.warning("âš ï¸ News filtering system disabled")
        logger.warning("   Place news_filter_simple.py in the same directory")
    
    # Redis ì´ˆê¸°í™”
    await init_redis()
    
    # Background tasks
    asyncio.create_task(health_check_monitor())
    asyncio.create_task(queue_processor())
    
    logger.info("="*70)
    logger.info("âœ… REXA server startup complete!")
    logger.info(f"   - Model: solar-mini")
    logger.info(f"   - RAG chunks: {len(chunk_embeddings)}")
    logger.info(f"   - Redis: {'connected' if redis_client else 'in-memory queue'}")
    logger.info(f"   - News API: {'enabled' if NAVER_CLIENT_ID else 'disabled'}")
    logger.info(f"   - News Filter: {'enabled' if NEWS_FILTER_AVAILABLE else 'disabled'}")
    logger.info("="*70)

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup resources on shutdown"""
    logger.info("ğŸ‘‹ Shutting down REXA server (Solar + RAG + News)...")
    await close_redis()
    logger.info("âœ… REXA server shut down successfully")
